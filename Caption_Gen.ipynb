{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1bf2fe-acad-436a-a13c-345b6fbbd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Captioning Model\n",
    "# This notebook implements an image captioning model using deep learning\n",
    "# The model takes an image as input and generates a natural language description\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29493193-cfea-4ad8-8e06-d381702963c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check for GPU availability\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Is GPU available: {tf.test.is_gpu_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec444e-f9af-43dc-8d80-e428240164f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    \"\"\"Configuration class to store model and training parameters\"\"\"\n",
    "    # Data paths\n",
    "    CAPTIONS_PATH = \"archive/captions.txt\"\n",
    "    IMAGES_PATH = \"archive/Images\"\n",
    "    GLOVE_PATH = \"glove.6B.200d.txt\"\n",
    "    \n",
    "    # Model parameters\n",
    "    EMBEDDING_DIM = 200  # GloVe embedding dimension\n",
    "    LSTM_UNITS = 256\n",
    "    DROPOUT_RATE = 0.5\n",
    "    MIN_WORD_FREQ = 8  # Minimum word frequency to include in vocabulary\n",
    "    \n",
    "    # Data split\n",
    "    TRAIN_SIZE = 0.8\n",
    "    VAL_SIZE = 0.1\n",
    "    TEST_SIZE = 0.1\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    INITIAL_LR = 0.001\n",
    "    LR_DECAY = 0.9\n",
    "    \n",
    "    # Caption generation\n",
    "    MAX_LENGTH = None  # Will be set dynamically based on data\n",
    "    BEAM_SIZE = 3\n",
    "    \n",
    "    # Paths for saving\n",
    "    MODEL_SAVE_PATH = \"model_weights/model_\"\n",
    "    PICKLE_PATH = \"pickles/\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create necessary directories\n",
    "        os.makedirs(\"model_weights\", exist_ok=True)\n",
    "        os.makedirs(\"pickles\", exist_ok=True)\n",
    "\n",
    "# Create config object\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4052e-e92d-4eb9-b21b-d0fc51e431fc",
   "metadata": {},
   "source": [
    "# ========================= 1. Data Loading and Preprocessing ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d851eed-4080-4001-89ec-cec9b46a9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_captions(text):\n",
    "    \"\"\"Extract image captions from text file\n",
    "    \n",
    "    Args:\n",
    "        text (str): Content of the captions file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping image IDs to their captions\n",
    "    \"\"\"\n",
    "    captions = {}\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # The file is in format image,caption\n",
    "        sentences = line.split(\",\") \n",
    "        if not sentences or len(sentences) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Remove .jpg from the image filename\n",
    "        img_id = sentences[0].split(\".\")[0]\n",
    "        \n",
    "        # Skip header row\n",
    "        if img_id == 'image':\n",
    "            continue\n",
    "            \n",
    "        # Initialize list for this image if not already present\n",
    "        if img_id not in captions:\n",
    "            captions[img_id] = []\n",
    "            \n",
    "        # Join the rest of the parts as the caption (in case caption contains commas)\n",
    "        captions[img_id].append(\",\".join(sentences[1:]).strip())\n",
    "    \n",
    "    return captions\n",
    "\n",
    "# Load captions from file\n",
    "try:\n",
    "    text = open(config.CAPTIONS_PATH, 'r', encoding='utf-8').read()\n",
    "    captions = map_captions(text)\n",
    "    print(f\"Loaded captions for {len(captions)} images\")\n",
    "    print(f\"Sample caption for image '1000268201_693b08cb0e': {captions['1000268201_693b08cb0e'][0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading captions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2d1d7-3a0c-41f7-b335-0c3ad14f930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions):\n",
    "    \"\"\"Clean and preprocess captions by:\n",
    "    1. Adding start/end sequence tokens\n",
    "    2. Converting to lowercase\n",
    "    3. Removing punctuation\n",
    "    4. Removing short words\n",
    "    5. Removing non-alphabetic words\n",
    "    \n",
    "    Args:\n",
    "        captions (dict): Dictionary mapping image IDs to their captions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with processed captions\n",
    "    \"\"\"\n",
    "    # Create translation table to remove punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Process each caption\n",
    "    for img_id, caption_list in captions.items():\n",
    "        for i in range(len(caption_list)):\n",
    "            caption = caption_list[i]\n",
    "            \n",
    "            # Add start and end sequence tokens\n",
    "            caption = 'startseq ' + caption + ' endseq'\n",
    "            \n",
    "            # Convert to list of words\n",
    "            words = caption.split()\n",
    "            \n",
    "            # Clean words\n",
    "            words = [word.lower() for word in words]  # lowercase\n",
    "            words = [word.translate(punc_table) for word in words]  # remove punctuation\n",
    "            words = [word for word in words if len(word) > 1]  # remove 1-letter words\n",
    "            words = [word for word in words if word.isalpha()]  # remove words with numbers\n",
    "            \n",
    "            # Convert back to string\n",
    "            caption_list[i] = ' '.join(words)\n",
    "    \n",
    "    return captions\n",
    "\n",
    "# Preprocess captions\n",
    "processed_captions = preprocess_captions(captions.copy())\n",
    "print(f\"Sample processed caption: {processed_captions['1000268201_693b08cb0e'][0]}\")\n",
    "\n",
    "# Save processed captions\n",
    "os.makedirs(config.PICKLE_PATH, exist_ok=True)\n",
    "dump(processed_captions, open(f\"{config.PICKLE_PATH}captions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd902080-458e-43d3-9124-a4341c35ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(captions, min_word_freq=1):\n",
    "    \"\"\"Create vocabulary from captions with minimum frequency filtering\n",
    "    \n",
    "    Args:\n",
    "        captions (dict): Dictionary mapping image IDs to their captions\n",
    "        min_word_freq (int): Minimum word frequency to include in vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vocabulary set, word frequency dictionary)\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_freq = {}\n",
    "    for img_id in captions.keys():\n",
    "        for caption in captions[img_id]:\n",
    "            for word in caption.split():\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 0\n",
    "                word_freq[word] += 1\n",
    "    \n",
    "    # Filter by frequency\n",
    "    vocab = [word for word in word_freq.keys() if word_freq[word] >= min_word_freq]\n",
    "    \n",
    "    return set(vocab), word_freq\n",
    "\n",
    "# Create vocabulary\n",
    "vocab, word_freq = create_vocabulary(processed_captions, config.MIN_WORD_FREQ)\n",
    "print(f\"Total unique words: {len(word_freq)}\")\n",
    "print(f\"Vocabulary size after frequency filtering ({config.MIN_WORD_FREQ}+ occurrences): {len(vocab)}\")\n",
    "print(f\"Some common words: {list(vocab)[:5]}\")\n",
    "\n",
    "# Visualize word frequency distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([freq for freq in word_freq.values() if freq < 100], bins=30)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0912396-133b-494e-b306-4fd21c712d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word-to-index and index-to-word mappings\n",
    "def build_word_mappings(vocab):\n",
    "    \"\"\"Build word-to-index and index-to-word mappings\n",
    "    \n",
    "    Args:\n",
    "        vocab (set): Set of words in vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (word_to_idx dict, idx_to_word dict, vocab_size)\n",
    "    \"\"\"\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "    \n",
    "    # Reserve 0 for padding\n",
    "    index = 1\n",
    "    for word in vocab:\n",
    "        word_to_idx[word] = index\n",
    "        idx_to_word[index] = word\n",
    "        index += 1\n",
    "    \n",
    "    # Add unknown token\n",
    "    word_to_idx['<unk>'] = index\n",
    "    idx_to_word[index] = '<unk>'\n",
    "    index += 1\n",
    "    \n",
    "    return word_to_idx, idx_to_word, index  # index is the vocab size\n",
    "\n",
    "# Build mappings\n",
    "word_to_idx, idx_to_word, vocab_size = build_word_mappings(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample word to index: {list(word_to_idx.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53267bd6-95c6-48f1-be84-e82502c3fb27",
   "metadata": {},
   "source": [
    "# ========================= 2. Image Processing ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e07ae-d983-4400-a1f8-eb4fb4a695a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all image paths\n",
    "img_paths = list(glob.glob(f\"{config.IMAGES_PATH}/*.jpg\"))\n",
    "print(f\"Found {len(img_paths)} images\")\n",
    "\n",
    "# Filter to only include images that have captions\n",
    "filtered_img_paths = []\n",
    "for path in img_paths:\n",
    "    img_id = os.path.basename(path).split('.')[0]\n",
    "    if img_id in processed_captions:\n",
    "        filtered_img_paths.append(path)\n",
    "\n",
    "print(f\"Filtered to {len(filtered_img_paths)} images with captions\")\n",
    "\n",
    "# Shuffle and split data\n",
    "random.shuffle(filtered_img_paths)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(len(filtered_img_paths) * config.TRAIN_SIZE)\n",
    "val_size = int(len(filtered_img_paths) * config.VAL_SIZE)\n",
    "\n",
    "# Split paths\n",
    "train_paths = filtered_img_paths[:train_size]\n",
    "val_paths = filtered_img_paths[train_size:train_size+val_size]\n",
    "test_paths = filtered_img_paths[train_size+val_size:]\n",
    "\n",
    "print(f\"Train set: {len(train_paths)} images\")\n",
    "print(f\"Validation set: {len(val_paths)} images\")\n",
    "print(f\"Test set: {len(test_paths)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91949f2b-e393-4552-ac9b-006f62b2d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries mapping dataset splits to image IDs\n",
    "def extract_ids_from_paths(paths, base_path):\n",
    "    \"\"\"Extract image IDs from file paths\n",
    "    \n",
    "    Args:\n",
    "        paths (list): List of image file paths\n",
    "        base_path (str): Base path to remove from file paths\n",
    "        \n",
    "    Returns:\n",
    "        list: List of image IDs\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for path in paths:\n",
    "        # Get filename without extension\n",
    "        filename = os.path.basename(path)\n",
    "        img_id = filename.split('.')[0]\n",
    "        result.append(img_id)\n",
    "    return result\n",
    "\n",
    "# Extract image IDs\n",
    "train_ids = extract_ids_from_paths(train_paths, config.IMAGES_PATH)\n",
    "val_ids = extract_ids_from_paths(val_paths, config.IMAGES_PATH)\n",
    "test_ids = extract_ids_from_paths(test_paths, config.IMAGES_PATH)\n",
    "\n",
    "# Create dictionaries for each split\n",
    "train_captions = {img_id: processed_captions[img_id] for img_id in train_ids}\n",
    "val_captions = {img_id: processed_captions[img_id] for img_id in val_ids}\n",
    "test_captions = {img_id: processed_captions[img_id] for img_id in test_ids}\n",
    "\n",
    "print(f\"Train captions: {len(train_captions)}\")\n",
    "print(f\"Validation captions: {len(val_captions)}\")\n",
    "print(f\"Test captions: {len(test_captions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee540c-2907-4cc3-8366-3b96f0707197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save caption dictionaries\n",
    "dump(train_captions, open(f\"{config.PICKLE_PATH}train_captions.pkl\", \"wb\"))\n",
    "dump(val_captions, open(f\"{config.PICKLE_PATH}val_captions.pkl\", \"wb\"))\n",
    "dump(test_captions, open(f\"{config.PICKLE_PATH}test_captions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e271c5-577b-4a3f-bb94-c559338f9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine maximum caption length for padding\n",
    "def get_max_length(captions_dict):\n",
    "    \"\"\"Calculate maximum caption length\n",
    "    \n",
    "    Args:\n",
    "        captions_dict (dict): Dictionary of captions\n",
    "        \n",
    "    Returns:\n",
    "        int: Maximum caption length\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for img_id, captions_list in captions_dict.items():\n",
    "        for caption in captions_list:\n",
    "            length = len(caption.split())\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "    return max_length\n",
    "\n",
    "config.MAX_LENGTH = get_max_length(train_captions)\n",
    "print(f\"Maximum caption length: {config.MAX_LENGTH} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4c918-c8cb-467c-a036-c03b2fdc5516",
   "metadata": {},
   "source": [
    "# ========================= 3. Feature Extraction ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d592197-bf60-4190-98b7-71ce09f412a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for image processing\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61523f-a372-489b-988b-0b952e7de42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InceptionV3 model pre-trained on ImageNet\n",
    "def load_image_model():\n",
    "    \"\"\"Load and prepare InceptionV3 model for feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        Model: Keras model for feature extraction\n",
    "    \"\"\"\n",
    "    # Load InceptionV3 with pre-trained weights\n",
    "    base_model = InceptionV3(weights='imagenet')\n",
    "    \n",
    "    # Remove the classification layer (the last fully connected layer)\n",
    "    # We use the output of the second-to-last layer as our image features\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "image_model = load_image_model()\n",
    "print(\"InceptionV3 model loaded for feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c2cf2-0a06-480d-a1b3-2340f0fc86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image for InceptionV3\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Preprocessed image\n",
    "    \"\"\"\n",
    "    # Load image at target size\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    \n",
    "    # Convert to array\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    # Expand dimensions to create batch of size 1\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Preprocess for InceptionV3\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    \"\"\"Extract features from an image using InceptionV3\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        model (Model): Keras model for feature extraction\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Feature vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the image\n",
    "        img = preprocess_image(image_path)\n",
    "        \n",
    "        # Extract features\n",
    "        features = model.predict(img, verbose=0)\n",
    "        \n",
    "        # Reshape from (1, 2048) to (2048,)\n",
    "        features = features.reshape(features.shape[1])\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f73a18-8d7a-4ab1-9bb8-542f351f05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all images in train, validation, and test sets\n",
    "def extract_features_for_dataset(image_paths, base_path, model):\n",
    "    \"\"\"Extract features for a list of images\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        base_path (str): Base path to remove from file paths\n",
    "        model (Model): Keras model for feature extraction\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping image IDs to feature vectors\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for path in tqdm(image_paths, desc=\"Extracting features\"):\n",
    "        # Get image ID\n",
    "        img_id = os.path.basename(path).split('.')[0]\n",
    "        \n",
    "        # Extract features\n",
    "        img_features = extract_features(path, model)\n",
    "        \n",
    "        if img_features is not None:\n",
    "            features[img_id] = img_features\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ask user if they want to extract features (can be time-consuming)\n",
    "print(\"Feature extraction can take a long time. Do you want to proceed? (yes/no)\")\n",
    "# This would normally be an input() call, but for this notebook we'll assume yes\n",
    "response = \"yes\"  # input()\n",
    "\n",
    "if response.lower() == 'yes':\n",
    "    # Extract features\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract for train set\n",
    "    train_features = extract_features_for_dataset(train_paths, config.IMAGES_PATH, image_model)\n",
    "    print(f\"Extracted features for {len(train_features)} training images\")\n",
    "    \n",
    "    # Extract for validation set\n",
    "    val_features = extract_features_for_dataset(val_paths, config.IMAGES_PATH, image_model)\n",
    "    print(f\"Extracted features for {len(val_features)} validation images\")\n",
    "    \n",
    "    # Extract for test set\n",
    "    test_features = extract_features_for_dataset(test_paths, config.IMAGES_PATH, image_model)\n",
    "    print(f\"Extracted features for {len(test_features)} test images\")\n",
    "    \n",
    "    # Save features\n",
    "    dump(train_features, open(f\"{config.PICKLE_PATH}train_features.pkl\", \"wb\"))\n",
    "    dump(val_features, open(f\"{config.PICKLE_PATH}val_features.pkl\", \"wb\"))\n",
    "    dump(test_features, open(f\"{config.PICKLE_PATH}test_features.pkl\", \"wb\"))\n",
    "    \n",
    "    # Print time taken\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Feature extraction completed in {elapsed_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"Skipping feature extraction. You can load pre-extracted features if available.\")\n",
    "    \n",
    "    # For demonstration purposes only - in a real notebook, you'd load actual features\n",
    "    # Here we're creating dummy features\n",
    "    def create_dummy_features(ids):\n",
    "        return {img_id: np.random.randn(2048) for img_id in ids}\n",
    "    \n",
    "    train_features = create_dummy_features(train_ids)\n",
    "    val_features = create_dummy_features(val_ids)\n",
    "    test_features = create_dummy_features(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33936b4c-1fd1-4893-a7a1-2a20c218e38b",
   "metadata": {},
   "source": [
    "# ========================= 4. Word Embeddings ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a46192-b1ac-4c1f-be8b-68a1a7bf28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(glove_path, embedding_dim):\n",
    "    \"\"\"Load GloVe embeddings from file\n",
    "    \n",
    "    Args:\n",
    "        glove_path (str): Path to GloVe file\n",
    "        embedding_dim (int): Embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping words to embedding vectors\n",
    "    \"\"\"\n",
    "    # Initialize empty dictionary\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    try:\n",
    "        # Open GloVe file\n",
    "        with open(glove_path, encoding=\"utf-8\") as f:\n",
    "            # Process each line\n",
    "            for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "                \n",
    "        print(f\"Loaded {len(embeddings_index)} word vectors\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GloVe embeddings: {e}\")\n",
    "        print(\"Creating random embeddings instead\")\n",
    "        \n",
    "        # Create random embeddings for the first 10000 common words\n",
    "        common_words = ['the', 'and', 'a', 'to', 'in', 'is', 'of', 'that', 'it', 'with']\n",
    "        for word in common_words:\n",
    "            embeddings_index[word] = np.random.uniform(-0.1, 0.1, embedding_dim)\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(config.GLOVE_PATH, config.EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50895b0a-167b-45db-b4d3-0473bf3c96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix for vocabulary\n",
    "def create_embedding_matrix(word_to_idx, embeddings_index, embedding_dim):\n",
    "    \"\"\"Create embedding matrix for vocabulary\n",
    "    \n",
    "    Args:\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        embeddings_index (dict): Dictionary mapping words to embedding vectors\n",
    "        embedding_dim (int): Embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Embedding matrix\n",
    "    \"\"\"\n",
    "    # Initialize embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros((len(word_to_idx) + 1, embedding_dim))\n",
    "    \n",
    "    # Fill embedding matrix with GloVe vectors\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        # Get embedding vector for word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        # If embedding vector exists, add it to embedding matrix\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            # Initialize with random values for unknown words\n",
    "            embedding_matrix[idx] = np.random.uniform(-0.1, 0.1, embedding_dim)\n",
    "            misses += 1\n",
    "    \n",
    "    print(f\"Found embeddings for {hits} words\")\n",
    "    print(f\"Missing embeddings for {misses} words\")\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(word_to_idx, glove_embeddings, config.EMBEDDING_DIM)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Save embedding matrix\n",
    "dump(embedding_matrix, open(f\"{config.PICKLE_PATH}embedding_matrix.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e761d-ef92-4b46-8bbc-6acbae886b5d",
   "metadata": {},
   "source": [
    "# ========================= 5. Data Generator ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d3995-c928-4f45-b78a-d2f406921933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Data generator for training the image captioning model\"\"\"\n",
    "    \n",
    "    def __init__(self, captions, features, word_to_idx, max_length, batch_size, shuffle=True):\n",
    "        \"\"\"Initialize data generator\n",
    "        \n",
    "        Args:\n",
    "            captions (dict): Dictionary mapping image IDs to captions\n",
    "            features (dict): Dictionary mapping image IDs to feature vectors\n",
    "            word_to_idx (dict): Dictionary mapping words to indices\n",
    "            max_length (int): Maximum caption length for padding\n",
    "            batch_size (int): Batch size\n",
    "            shuffle (bool): Whether to shuffle the data\n",
    "        \"\"\"\n",
    "        self.captions = captions\n",
    "        self.features = features\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Create list of data samples for efficient batching\n",
    "        self.data_samples = []\n",
    "        for img_id, caption_list in self.captions.items():\n",
    "            if img_id in self.features:\n",
    "                # Get image features\n",
    "                img_vector = self.features[img_id]\n",
    "                \n",
    "                for caption in caption_list:\n",
    "                    # Convert words to indices\n",
    "                    encoded_caption = [word_to_idx.get(word, word_to_idx['<unk>']) \n",
    "                                     for word in caption.split()]\n",
    "                    \n",
    "                    # Create samples for partial sequences\n",
    "                    for i in range(1, len(encoded_caption)):\n",
    "                        # Partial sequence as input\n",
    "                        caption_in = encoded_caption[:i]\n",
    "                        # Next word as target\n",
    "                        caption_out = encoded_caption[i]\n",
    "                        \n",
    "                        # Add sample to list\n",
    "                        self.data_samples.append((img_id, img_vector, caption_in, caption_out))\n",
    "        \n",
    "        self.indices = np.arange(len(self.data_samples))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of batches\"\"\"\n",
    "        return int(np.ceil(len(self.data_samples) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a batch of data\"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Create batch\n",
    "        x1, x2, y = [], [], []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            # Get sample\n",
    "            img_id, img_vector, caption_in, caption_out = self.data_samples[idx]\n",
    "            \n",
    "            # Add to batch\n",
    "            x1.append(img_vector)\n",
    "            x2.append(caption_in)\n",
    "            y.append(caption_out)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        x1 = np.array(x1)\n",
    "        \n",
    "        # Pad sequences\n",
    "        x2 = pad_sequences(x2, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        # One-hot encode outputs\n",
    "        y = to_categorical(y, num_classes=len(self.word_to_idx) + 1)\n",
    "        \n",
    "        return [x1, x2], y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = DataGenerator(\n",
    "    captions=train_captions,\n",
    "    features=train_features,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    captions=val_captions,\n",
    "    features=val_features,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Created data generators with {len(train_generator)} training batches and {len(val_generator)} validation batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e33622-3293-43f2-ae04-fc107796f220",
   "metadata": {},
   "source": [
    "# ========================= 6. Model Architecture ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf5735-5515-4af4-8076-9e38cca301a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Embedding, LSTM, \n",
    "    BatchNormalization, add, concatenate, Bidirectional,\n",
    "    Attention, AdditiveAttention, MultiHeadAttention\n",
    ")\n",
    "\n",
    "def create_caption_model(vocab_size, max_length, embedding_dim, embedding_matrix):\n",
    "    \"\"\"Create image captioning model with attention\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Size of vocabulary\n",
    "        max_length (int): Maximum caption length\n",
    "        embedding_dim (int): Embedding dimension\n",
    "        embedding_matrix (ndarray): Pre-trained embedding matrix\n",
    "        \n",
    "    Returns:\n",
    "        Model: Keras model\n",
    "    \"\"\"\n",
    "    # Image feature input\n",
    "    image_input = Input(shape=(2048,))\n",
    "    image_dropout = Dropout(config.DROPOUT_RATE)(image_input)\n",
    "    image_dense = Dense(config.LSTM_UNITS, activation='relu')(image_dropout)\n",
    "    image_repeat = RepeatVector(max_length)(image_dense)\n",
    "    \n",
    "    # Caption input\n",
    "    caption_input = Input(shape=(max_length,))\n",
    "    caption_embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False  # Freeze embeddings initially\n",
    "    )(caption_input)\n",
    "    caption_dropout = Dropout(config.DROPOUT_RATE)(caption_embedding)\n",
    "    \n",
    "    # LSTM layer with attention mechanism\n",
    "    lstm = LSTM(config.LSTM_UNITS, return_sequences=True)(caption_dropout)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = AdditiveAttention()([lstm, image_repeat])\n",
    "    \n",
    "    # Combine attended caption with image features\n",
    "    decoder = concatenate([attention, image_repeat])\n",
    "    \n",
    "    # Process combined features\n",
    "    decoder = LSTM(config.LSTM_UNITS)(decoder)\n",
    "    decoder = Dropout(config.DROPOUT_RATE)(decoder)\n",
    "    decoder = Dense(config.LSTM_UNITS, activation='relu')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create caption model\n",
    "model = create_caption_model(\n",
    "    vocab_size=len(word_to_idx) + 1,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82aa34-f3f7-45bf-b1d0-789352214a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    \"\"\"Learning rate scheduler\"\"\"\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * config.LR_DECAY\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=config.MODEL_SAVE_PATH + '{epoch:02d}.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=config.INITIAL_LR),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0256e5-8f26-4b9b-ab63-bdaa15306ea6",
   "metadata": {},
   "source": [
    "# ========================= 7. Model Training ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac7574-a892-4b8a-9377-28a87acb218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=config.EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, early_stopping, lr_schedule, tensorboard],\n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b02422-9183-4633-8008-ac44587ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302714d-d034-42ee-b72d-471e5812319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze embedding layer and fine-tune model\n",
    "print(\"Fine-tuning model with unfrozen embedding layer\")\n",
    "\n",
    "# Unfreeze embedding layer\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "# Compile model with lower learning rate\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=config.INITIAL_LR * 0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model for a few more epochs\n",
    "history_ft = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, early_stopping, tensorboard],\n",
    "    workers=4,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(f\"{config.MODEL_SAVE_PATH}final.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9bd4d6-ba76-4208-9865-1eeaa896d8e5",
   "metadata": {},
   "source": [
    "# ========================= 8. Caption Generation ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697a536-9524-4216-913c-03194163c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model, image_features, word_to_idx, idx_to_word, max_length):\n",
    "    \"\"\"Generate caption using greedy search\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Trained model\n",
    "        image_features (ndarray): Image feature vector\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words\n",
    "        max_length (int): Maximum caption length\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    # Start with 'startseq'\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for i in range(max_length):\n",
    "        # Encode partial caption\n",
    "        sequence = [word_to_idx.get(word, word_to_idx['<unk>']) for word in in_text.split()]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        \n",
    "        # Predict next word\n",
    "        yhat = model.predict([np.array([image_features]), sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        \n",
    "        # Map prediction to word\n",
    "        word = idx_to_word.get(yhat, '<unk>')\n",
    "        \n",
    "        # Stop if end of sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "        # Append word to caption\n",
    "        in_text += ' ' + word\n",
    "    \n",
    "    # Remove start and end tokens\n",
    "    final = in_text.split()\n",
    "    final = final[1:]\n",
    "    if final[-1] == 'endseq':\n",
    "        final = final[:-1]\n",
    "    \n",
    "    # Join words to form caption\n",
    "    final = ' '.join(final)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb210269-acc6-41a3-afff-b1e04f557936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, image_features, word_to_idx, idx_to_word, max_length, beam_size=3):\n",
    "    \"\"\"Generate caption using beam search\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Trained model\n",
    "        image_features (ndarray): Image feature vector\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words\n",
    "        max_length (int): Maximum caption length\n",
    "        beam_size (int): Beam size\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (caption, probability) tuples\n",
    "    \"\"\"\n",
    "    # Start with 'startseq'\n",
    "    start_word = [word_to_idx['startseq']]\n",
    "    \n",
    "    # Initialize beam with start word\n",
    "    beam = [(start_word, 0.0)]\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for i in range(max_length - 1):\n",
    "        temp_beam = []\n",
    "        \n",
    "        for s, p in beam:\n",
    "            # If sequence ends with end token, keep it in beam\n",
    "            if s[-1] == word_to_idx.get('endseq'):\n",
    "                temp_beam.append((s, p))\n",
    "                continue\n",
    "            \n",
    "            # Pad sequence\n",
    "            sequence = pad_sequences([s], maxlen=max_length, padding='post')\n",
    "            \n",
    "            # Predict next word probabilities\n",
    "            preds = model.predict([np.array([image_features]), sequence], verbose=0)\n",
    "            \n",
    "            # Get top k predictions\n",
    "            top_preds = np.argsort(preds[0])[-beam_size:]\n",
    "            \n",
    "            # Create new sequences by appending each prediction\n",
    "            for w in top_preds:\n",
    "                next_seq = s.copy()\n",
    "                next_seq.append(w)\n",
    "                \n",
    "                # Calculate new score (log probability)\n",
    "                next_p = p + np.log(preds[0][w])\n",
    "                \n",
    "                # Add to temporary beam\n",
    "                temp_beam.append((next_seq, next_p))\n",
    "        \n",
    "        # Sort beam by score\n",
    "        temp_beam.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Keep only top beam_size sequences\n",
    "        beam = temp_beam[:beam_size]\n",
    "    \n",
    "    # Convert sequences to captions and calculate final scores\n",
    "    result = []\n",
    "    for sequence, score in beam:\n",
    "        # Convert sequence to words\n",
    "        words = [idx_to_word.get(idx, '<unk>') for idx in sequence]\n",
    "        \n",
    "        # Remove start and end tokens\n",
    "        if 'startseq' in words:\n",
    "            words.remove('startseq')\n",
    "        if 'endseq' in words:\n",
    "            words = words[:words.index('endseq')]\n",
    "        \n",
    "        # Join words to form caption\n",
    "        caption = ' '.join(words)\n",
    "        \n",
    "        # Convert log probability to probability\n",
    "        probability = np.exp(score)\n",
    "        \n",
    "        # Add to result\n",
    "        result.append((caption, probability))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95d8d1-85ab-47e2-ad9a-f633ea934209",
   "metadata": {},
   "source": [
    "# ========================= 9. Evaluation Metrics ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3e7b9-6897-43df-9c60-6d647e2f6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages for evaluation\n",
    "!pip install nltk pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eae03b-1501-41d7-87f3-b06efd952219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.rouge_score import rouge_n_sentence_level\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK data\n",
    "nitk.download('punkt')\n",
    "nitk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57800ba2-8fb8-4704-b046-42a5bc975a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, features, captions, word_to_idx, idx_to_word, max_length):\n",
    "    \"\"\"Evaluate model using various metrics\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Trained model\n",
    "        features (dict): Dictionary mapping image IDs to feature vectors\n",
    "        captions (dict): Dictionary mapping image IDs to captions\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words\n",
    "        max_length (int): Maximum caption length\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'bleu1': 0.0,\n",
    "        'bleu2': 0.0,\n",
    "        'bleu3': 0.0,\n",
    "        'bleu4': 0.0,\n",
    "        'meteor': 0.0,\n",
    "        'rouge_1': 0.0\n",
    "    }\n",
    "    \n",
    "    # Storage for all references and hypotheses\n",
    "    all_references = []\n",
    "    all_hypotheses = []\n",
    "    \n",
    "    # Generate captions for each image\n",
    "    for img_id, img_features in tqdm(list(features.items())[:100], desc=\"Evaluating\"):  # Limit to 100 images for speed\n",
    "        if img_id not in captions:\n",
    "            continue\n",
    "            \n",
    "        # Get reference captions\n",
    "        references = captions[img_id]\n",
    "        references = [caption.split() for caption in references]\n",
    "        \n",
    "        # Generate hypothesis caption\n",
    "        hypothesis = greedy_search(model, img_features, word_to_idx, idx_to_word, max_length)\n",
    "        hypothesis = hypothesis.split()\n",
    "        \n",
    "        # Add to lists\n",
    "        all_references.append(references)\n",
    "        all_hypotheses.append(hypothesis)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    metrics['bleu1'] = corpus_bleu(all_references, all_hypotheses, weights=(1.0, 0, 0, 0))\n",
    "    metrics['bleu2'] = corpus_bleu(all_references, all_hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    metrics['bleu3'] = corpus_bleu(all_references, all_hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    metrics['bleu4'] = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    # Calculate METEOR and ROUGE scores for a subset (these are more computationally expensive)\n",
    "    meteor_scores = []\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for i in range(min(25, len(all_hypotheses))):  # Limit to 25 images for meteor/rouge\n",
    "        # Join words back into strings for METEOR\n",
    "        hyp_str = ' '.join(all_hypotheses[i])\n",
    "        ref_strs = [' '.join(ref) for ref in all_references[i]]\n",
    "        \n",
    "        # Calculate METEOR\n",
    "        meteor_scores.append(meteor_score(ref_strs, hyp_str))\n",
    "        \n",
    "        # Calculate ROUGE-1\n",
    "        rouge_score = rouge_n_sentence_level(hyp_str, ref_strs[0], 1)  # Using first reference\n",
    "        rouge_scores.append(rouge_score)\n",
    "    \n",
    "    # Average scores\n",
    "    metrics['meteor'] = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "    metrics['rouge_1'] = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad162a-fccc-486a-b1b6-a812691f2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "test_metrics = evaluate_model(\n",
    "    model=model,\n",
    "    features=test_features,\n",
    "    captions=test_captions,\n",
    "    word_to_idx=word_to_idx,\n",
    "    idx_to_word=idx_to_word,\n",
    "    max_length=config.MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb094ba-d0b8-4d7f-8161-94068ada5b46",
   "metadata": {},
   "source": [
    "# ========================= 10. Visualization ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fba288-7590-46e5-878a-6e1974669dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, image_paths, features, captions, word_to_idx, idx_to_word, max_length, num_examples=5):\n",
    "    \"\"\"Visualize model predictions\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Trained model\n",
    "        image_paths (list): List of image file paths\n",
    "        features (dict): Dictionary mapping image IDs to feature vectors\n",
    "        captions (dict): Dictionary mapping image IDs to reference captions\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words\n",
    "        max_length (int): Maximum caption length\n",
    "        num_examples (int): Number of examples to visualize\n",
    "    \"\"\"\n",
    "    # Randomly select images\n",
    "    selected_indices = np.random.choice(len(image_paths), num_examples, replace=False)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        # Get image path\n",
    "        img_path = image_paths[idx]\n",
    "        img_id = os.path.basename(img_path).split('.')[0]\n",
    "        \n",
    "        # Skip if image not in features or captions\n",
    "        if img_id not in features or img_id not in captions:\n",
    "            continue\n",
    "        \n",
    "        # Get features and generate captions\n",
    "        img_features = features[img_id]\n",
    "        \n",
    "        # Generate caption using greedy search\n",
    "        greedy_caption = greedy_search(model, img_features, word_to_idx, idx_to_word, max_length)\n",
    "        \n",
    "        # Generate caption using beam search\n",
    "        beam_captions = beam_search(model, img_features, word_to_idx, idx_to_word, max_length, beam_size=3)\n",
    "        \n",
    "        # Get reference captions\n",
    "        ref_captions = captions[img_id]\n",
    "        \n",
    "        # Load and display image\n",
    "        plt.subplot(num_examples, 2, 2*i+1)\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display captions\n",
    "        plt.subplot(num_examples, 2, 2*i+2)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        caption_text = f\"Reference Captions:\\n\"\n",
    "        for j, ref in enumerate(ref_captions[:2]):  # Show only 2 reference captions\n",
    "            caption_text += f\"   {j+1}. {ref}\\n\"\n",
    "        \n",
    "        caption_text += f\"\\nGreedy Search:\\n   {greedy_caption}\\n\"\n",
    "        \n",
    "        caption_text += f\"\\nBeam Search (top 3):\\n\"\n",
    "        for j, (caption, prob) in enumerate(beam_captions[:3]):\n",
    "            caption_text += f\"   {j+1}. {caption} (p={prob:.4f})\\n\"\n",
    "        \n",
    "        plt.text(0, 0.5, caption_text, fontsize=10, verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c95f2-58ca-45eb-a058-be0e4d417c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test set\n",
    "visualize_predictions(\n",
    "    model=model,\n",
    "    image_paths=test_paths,\n",
    "    features=test_features,\n",
    "    captions=test_captions,\n",
    "    word_to_idx=word_to_idx,\n",
    "    idx_to_word=idx_to_word,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    num_examples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3223c9-2bd5-434d-a70f-d02cd8cfb328",
   "metadata": {},
   "source": [
    "# ========================= 11. Inference on New Images ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6821a4-5fc6-470e-8ee8-0ecdedaca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(model, image_path, word_to_idx, idx_to_word, max_length, image_model):\n",
    "    \"\"\"Generate caption for a new image\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Trained captioning model\n",
    "        image_path (str): Path to image file\n",
    "        word_to_idx (dict): Dictionary mapping words to indices\n",
    "        idx_to_word (dict): Dictionary mapping indices to words\n",
    "        max_length (int): Maximum caption length\n",
    "        image_model (Model): InceptionV3 model for feature extraction\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (greedy_caption, beam_captions)\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    img_features = extract_features(image_path, image_model)\n",
    "    \n",
    "    # Generate caption using greedy search\n",
    "    greedy_caption = greedy_search(model, img_features, word_to_idx, idx_to_word, max_length)\n",
    "    \n",
    "    # Generate caption using beam search\n",
    "    beam_captions = beam_search(model, img_features, word_to_idx, idx_to_word, max_length, beam_size=3)\n",
    "    \n",
    "    # Load and display image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    img = plt.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Print captions\n",
    "    print(\"Greedy Search:\")\n",
    "    print(f\"   {greedy_caption}\")\n",
    "    print(\"\\nBeam Search (top 3):\")\n",
    "    for i, (caption, prob) in enumerate(beam_captions):\n",
    "        print(f\"   {i+1}. {caption} (p={prob:.4f})\")\n",
    "    \n",
    "    plt.title(f\"Caption: {greedy_caption}\", fontsize=12, pad=10)\n",
    "    plt.show()\n",
    "    \n",
    "    return greedy_caption, beam_captions\n",
    "\n",
    "# Caption a sample test image\n",
    "sample_image_path = test_paths[0]\n",
    "if os.path.exists(sample_image_path):\n",
    "    captions = caption_image(\n",
    "        model=model,\n",
    "        image_path=sample_image_path,\n",
    "        word_to_idx=word_to_idx,\n",
    "        idx_to_word=idx_to_word,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        image_model=image_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d5ec8-1dc5-4726-93c3-129d6318b2dc",
   "metadata": {},
   "source": [
    "# ========================= 12. Save Models and Data ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae497c60-d877-4567-9928-f3453adfcbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary and mappings\n",
    "vocab_data = {\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'idx_to_word': idx_to_word,\n",
    "    'vocab_size': vocab_size,\n",
    "    'max_length': config.MAX_LENGTH\n",
    "}\n",
    "\n",
    "with open(f\"{config.PICKLE_PATH}vocabulary.json\", 'w') as f:\n",
    "    json.dump(vocab_data, f, indent=2)\n",
    "\n",
    "# Save model configuration\n",
    "config_data = {\n",
    "    'embedding_dim': config.EMBEDDING_DIM,\n",
    "    'lstm_units': config.LSTM_UNITS,\n",
    "    'dropout_rate': config.DROPOUT_RATE,\n",
    "    'max_length': config.MAX_LENGTH,\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "with open(f\"{config.PICKLE_PATH}config.json\", 'w') as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "\n",
    "# Save model\n",
    "model.save(f\"{config.MODEL_SAVE_PATH}final.keras\")\n",
    "print(\"Model and configuration saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedc20f-1a84-4ff9-ada0-d2cfba886e7a",
   "metadata": {},
   "source": [
    "# ========================= 13. Conclusion ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57429c2-232d-4ba5-b13f-519a9846b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image Captioning Project Summary\n",
    "\n",
    "In this notebook, we built an image captioning model that generates descriptive captions for images.\n",
    "\n",
    "Key components of the project:\n",
    "\n",
    "1. Data Processing:\n",
    "   - Loaded and preprocessed image captions\n",
    "   - Built vocabulary with frequency filtering\n",
    "   - Created train/validation/test splits\n",
    "\n",
    "2. Feature Extraction:\n",
    "   - Used pre-trained InceptionV3 model to extract image features\n",
    "   - Stored features for efficient training\n",
    "\n",
    "3. Model Architecture:\n",
    "   - Implemented an encoder-decoder model with attention mechanism\n",
    "   - Used pre-trained GloVe word embeddings\n",
    "   - LSTM-based decoder with attention on image features\n",
    "\n",
    "4. Training:\n",
    "   - Custom data generator for efficient training\n",
    "   - Learning rate scheduling and early stopping\n",
    "   - Fine-tuning with unfrozen embeddings\n",
    "\n",
    "5. Caption Generation:\n",
    "   - Implemented both greedy search and beam search algorithms\n",
    "   - Beam search provides multiple caption candidates\n",
    "\n",
    "6. Evaluation:\n",
    "   - Used BLEU, METEOR, and ROUGE metrics\n",
    "   - Visualized model predictions\n",
    "\n",
    "Areas for improvement:\n",
    "   - Experiment with different CNN architectures (ResNet, EfficientNet)\n",
    "   - Try Transformer-based models instead of LSTM\n",
    "   - Incorporate more training data\n",
    "   - Implement advanced attention mechanisms\n",
    "   - Fine-tune the CNN backbone for better features\n",
    "\"\"\"\n",
    "\n",
    "print(\"Image Captioning Project Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd163a-83a8-4332-bfdb-764633cfb94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e4947-12a5-41f5-bb82-d371a2cd3fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83198123-ab42-4847-8249-b87bee5ed7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb9cb5-5410-4704-8a7a-54dfc2facd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ab1e3-1272-490d-bad6-750f08a865b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc285491-bc03-4945-add9-d13b1d398b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa306f-1b45-4cea-837f-bfed9d70440c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc54fe2-927a-4233-9ffe-8c40b79f9b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d34ad-e564-4d63-8925-87fafc8c7486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc40f3-bd07-497f-9caf-40e0f0d3c470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5a3e-e248-410d-bf91-899a839423e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e2081-51c8-4869-8dc3-ecb4202a788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f7bef-1aea-43ea-90ad-667312c53977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6950fcf-72d2-4cce-83c4-49f03c0670a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
